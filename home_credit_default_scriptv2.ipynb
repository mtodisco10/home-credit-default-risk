{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Data\n",
    "train = pd.read_csv('dataFiles/application_train.csv')\n",
    "test = pd.read_csv('dataFiles/application_test.csv')\n",
    "bureau_data = pd.read_csv('dataFiles/bureau.csv')\n",
    "bureau_balance_data = pd.read_csv('dataFiles/bureau_balance.csv')\n",
    "prev_app_data = pd.read_csv('dataFiles/previous_application.csv')\n",
    "pos_cash_balance_data = pd.read_csv('dataFiles/POS_CASH_balance.csv')\n",
    "installments_data = pd.read_csv('dataFiles/installments_payments.csv')\n",
    "cc_data = pd.read_csv('dataFiles/credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.select_dtypes(exclude='object')\n",
    "test_num = test.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformations\n",
    "train_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',\\\n",
    "       'AMT_GOODS_PRICE']] = np.log(train_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']])\n",
    "\n",
    "test_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',\\\n",
    "       'AMT_GOODS_PRICE']] = np.log(test_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clipping Outliers\n",
    "train_num['CNT_CHILDREN'] = train_num['CNT_CHILDREN'].apply(lambda x: 5 if x >=5 else x)\n",
    "train_num['CNT_FAM_MEMBERS'] = train_num['CNT_FAM_MEMBERS'].apply(lambda x: 5 if x >=5 else x)\n",
    "train_num['AMT_INCOME_TOTAL'] = train_num['AMT_INCOME_TOTAL'].apply(lambda x: 14 if x >=14 else x)\n",
    "train_num['HOUR_APPR_PROCESS_START'] = train_num['HOUR_APPR_PROCESS_START'].apply(lambda x: 2 if x <= 2 else x)\n",
    "train_num['COMMONAREA_AVG'] = train_num['COMMONAREA_AVG'].fillna(0).apply(lambda x: 0.3 if x >= 0.3 else x)\n",
    "train_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = train_num['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda x: 9 if x >= 9 else x).fillna(10)\n",
    "\n",
    "test_num['CNT_CHILDREN'] = test_num['CNT_CHILDREN'].apply(lambda x: 5 if x >=5 else x)\n",
    "test_num['CNT_FAM_MEMBERS'] = test_num['CNT_FAM_MEMBERS'].apply(lambda x: 5 if x >=5 else x)\n",
    "test_num['AMT_INCOME_TOTAL'] = test_num['AMT_INCOME_TOTAL'].apply(lambda x: 14 if x >=14 else x)\n",
    "test_num['HOUR_APPR_PROCESS_START'] = test_num['HOUR_APPR_PROCESS_START'].apply(lambda x: 2 if x <= 2 else x)\n",
    "test_num['COMMONAREA_AVG'] = test_num['COMMONAREA_AVG'].fillna(0).apply(lambda x: 0.3 if x >= 0.3 else x)\n",
    "test_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = test_num['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda x: 9 if x >= 9 else x).fillna(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINNING THE TWO HUMPS FROM DAYS_EMPLOYED\n",
    "train_num['DAYS_EMPLOYED_BIN_1'] = train_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x < 150000 else 0)\n",
    "train_num['DAYS_EMPLOYED_BIN_2'] = train_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x >= 150000 else 0)\n",
    "\n",
    "#BINNING THE TWO HUMPS FROM DAYS_EMPLOYED\n",
    "test_num['DAYS_EMPLOYED_BIN_1'] = test_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x < 150000 else 0)\n",
    "test_num['DAYS_EMPLOYED_BIN_2'] = test_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x >= 150000 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABS & BOX COX Transformation\n",
    "train_num['DAYS_REGISTRATION'] = np.abs(train_num['DAYS_REGISTRATION'])\n",
    "train_num['DAYS_REGISTRATION'] = train_num['DAYS_REGISTRATION'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "train_num['DAYS_REGISTRATION'] = boxcox(train_num['DAYS_REGISTRATION'],0.5)\n",
    "\n",
    "train_num['DAYS_ID_PUBLISH'] = np.abs(train_num['DAYS_ID_PUBLISH'])\n",
    "train_num['DAYS_ID_PUBLISH'] = train_num['DAYS_ID_PUBLISH'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "train_num['DAYS_ID_PUBLISH'] = boxcox(train_num['DAYS_ID_PUBLISH'],0.5)\n",
    "\n",
    "train_num['ENTRANCES_AVG'] = train_num['ENTRANCES_AVG'].apply(lambda x: np.sqrt(x) if pd.notnull(x) else x)\n",
    "\n",
    "test_num['DAYS_REGISTRATION'] = np.abs(test_num['DAYS_REGISTRATION'])\n",
    "test_num['DAYS_REGISTRATION'] = test_num['DAYS_REGISTRATION'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "test_num['DAYS_REGISTRATION'] = boxcox(test_num['DAYS_REGISTRATION'],0.5)\n",
    "\n",
    "test_num['DAYS_ID_PUBLISH'] = np.abs(test_num['DAYS_ID_PUBLISH'])\n",
    "test_num['DAYS_ID_PUBLISH'] = test_num['DAYS_ID_PUBLISH'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "test_num['DAYS_ID_PUBLISH'] = boxcox(test_num['DAYS_ID_PUBLISH'],0.5)\n",
    "\n",
    "test_num['ENTRANCES_AVG'] = test_num['ENTRANCES_AVG'].apply(lambda x: np.sqrt(x) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Bins for Own Car Age\n",
    "def own_car_age_bins(x):\n",
    "    if pd.isnull(x):\n",
    "        return 0\n",
    "    elif x <= 10:\n",
    "        return 1\n",
    "    elif x <= 30:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "train['OWN_CAR_AGE_BINS'] = train['OWN_CAR_AGE'].apply(own_car_age_bins)\n",
    "\n",
    "test['OWN_CAR_AGE_BINS'] = test['OWN_CAR_AGE'].apply(own_car_age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to drop because of sparse features\n",
    "train_num = train_num.drop(['FLAG_MOBIL','FLAG_CONT_MOBILE','FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\\\n",
    "                            'FLAG_DOCUMENT_7','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_15',\\\n",
    "                            'FLAG_DOCUMENT_17','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\\\n",
    "                            'OWN_CAR_AGE','DAYS_EMPLOYED','DAYS_EMPLOYED_BIN_1','AMT_REQ_CREDIT_BUREAU_HOUR',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_QRT'], axis = 1)\n",
    "\n",
    "test_num = test_num.drop(['FLAG_MOBIL','FLAG_CONT_MOBILE','FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\\\n",
    "                          'FLAG_DOCUMENT_7','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_15',\\\n",
    "                          'FLAG_DOCUMENT_17','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\\\n",
    "                          'OWN_CAR_AGE','DAYS_EMPLOYED','DAYS_EMPLOYED_BIN_1','AMT_REQ_CREDIT_BUREAU_HOUR',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_QRT'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FillNA Columns\n",
    "train_num.iloc[:,25:76] = train_num.iloc[:,25:76].fillna(0).astype(int)\n",
    "train_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = train_num['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(11).astype(int)\n",
    "\n",
    "test_num.iloc[:,24:75] = test_num.iloc[:,24:75].fillna(0).astype(int)\n",
    "test_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = test_num['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(11).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Median Columns\n",
    "train_num.loc[:,'AMT_ANNUITY'] = train_num.loc[:,'AMT_ANNUITY'].fillna(train_num.AMT_ANNUITY.median())\n",
    "train_num.loc[:,'AMT_GOODS_PRICE'] = train_num.loc[:,'AMT_GOODS_PRICE'].fillna(train_num.AMT_GOODS_PRICE.median())\n",
    "train_num.loc[:,'CNT_FAM_MEMBERS'] = train_num.loc[:,'CNT_FAM_MEMBERS'].fillna(train_num.CNT_FAM_MEMBERS.median())\n",
    "\n",
    "test_num.loc[:,'AMT_ANNUITY'] = test_num.loc[:,'AMT_ANNUITY'].fillna(test_num.AMT_ANNUITY.median())\n",
    "test_num.loc[:,'AMT_GOODS_PRICE'] = test_num.loc[:,'AMT_GOODS_PRICE'].fillna(test_num.AMT_GOODS_PRICE.median())\n",
    "test_num.loc[:,'CNT_FAM_MEMBERS'] = test_num.loc[:,'CNT_FAM_MEMBERS'].fillna(test_num.CNT_FAM_MEMBERS.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num['children_ratio'] = train_num['CNT_CHILDREN'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['credit_to_annuity_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_ANNUITY']\n",
    "train_num['credit_to_goods_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_GOODS_PRICE']\n",
    "train_num['credit_to_income_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_INCOME_TOTAL']\n",
    "train_num['income_credit_percentage'] = train_num['AMT_INCOME_TOTAL'] / train_num['AMT_CREDIT']\n",
    "train_num['income_per_child'] = train_num['AMT_INCOME_TOTAL'] / (1 + train_num['CNT_CHILDREN'])\n",
    "train_num['income_per_person'] = train_num['AMT_INCOME_TOTAL'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['payment_rate'] = train_num['AMT_ANNUITY'] / train_num['AMT_CREDIT']\n",
    "train_num['phone_to_birth_ratio'] = train_num['DAYS_LAST_PHONE_CHANGE'] / train_num['DAYS_BIRTH']\n",
    "train_num['cnt_non_child'] = train_num['CNT_FAM_MEMBERS'] - train_num['CNT_CHILDREN']\n",
    "train_num['child_to_non_child_ratio'] = train_num['CNT_CHILDREN'] / (1+ train_num['cnt_non_child'])\n",
    "train_num['income_per_non_child'] = train_num['AMT_INCOME_TOTAL'] / (1 + train_num['cnt_non_child'])\n",
    "train_num['credit_per_person'] = train_num['AMT_CREDIT'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['credit_per_child'] = train_num['AMT_CREDIT'] / (1 + train_num['CNT_CHILDREN'])\n",
    "train_num['credit_per_non_child'] = train_num['AMT_CREDIT'] / (1 + train_num['cnt_non_child'])\n",
    "\n",
    "test_num['children_ratio'] = test_num['CNT_CHILDREN'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['credit_to_annuity_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_ANNUITY']\n",
    "test_num['credit_to_goods_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_GOODS_PRICE']\n",
    "test_num['credit_to_income_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_INCOME_TOTAL']\n",
    "test_num['income_credit_percentage'] = test_num['AMT_INCOME_TOTAL'] / test_num['AMT_CREDIT']\n",
    "test_num['income_per_child'] = test_num['AMT_INCOME_TOTAL'] / (1 + test_num['CNT_CHILDREN'])\n",
    "test_num['income_per_person'] = test_num['AMT_INCOME_TOTAL'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['payment_rate'] = test_num['AMT_ANNUITY'] / test_num['AMT_CREDIT']\n",
    "test_num['phone_to_birth_ratio'] = test_num['DAYS_LAST_PHONE_CHANGE'] / test_num['DAYS_BIRTH']\n",
    "test_num['cnt_non_child'] = test_num['CNT_FAM_MEMBERS'] - test_num['CNT_CHILDREN']\n",
    "test_num['child_to_non_child_ratio'] = test_num['CNT_CHILDREN'] / (1 + test_num['cnt_non_child'])\n",
    "test_num['income_per_non_child'] = test_num['AMT_INCOME_TOTAL'] / (1 + test_num['cnt_non_child'])\n",
    "test_num['credit_per_person'] = test_num['AMT_CREDIT'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['credit_per_child'] = test_num['AMT_CREDIT'] / (1 + test_num['CNT_CHILDREN'])\n",
    "test_num['credit_per_non_child'] = test_num['AMT_CREDIT'] / (1 + test_num['cnt_non_child']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Categorical Variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_cat = train.select_dtypes(include='object').fillna('Missing')\n",
    "test_cat = test.select_dtypes(include='object').fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that are sparse\n",
    "train_cat = train_cat.drop('ORGANIZATION_TYPE', axis = 1)\n",
    "test_cat = test_cat.drop('ORGANIZATION_TYPE', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "train_cat = train_cat.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "test_cat = test_cat.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating Train & Test Numerical & Categorical Features\n",
    "train_clean = pd.concat([train_num, train_cat], axis = 1)\n",
    "\n",
    "test_clean = pd.concat([test_num, test_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_sample(train_num_scaled, train.TARGET)\n",
    "\n",
    "# from collections import Counter\n",
    "# print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bureau Balance ####\n",
    "bureau_balance_data_grouped = pd.get_dummies(bureau_balance_data).groupby('SK_ID_BUREAU', as_index=False).agg({'STATUS_1':'count','MONTHS_BALANCE':min,\\\n",
    "                                                                                 'STATUS_C':sum,'STATUS_0':sum,'STATUS_X':sum})\n",
    "\n",
    "bureau_balance_data_grouped = bureau_balance_data_grouped.rename(columns={'STATUS_1':'BALANCE_COUNT'})\n",
    "\n",
    "bureau_balance_data_grouped['STATUS_X_RATIO'] = bureau_balance_data_grouped['STATUS_X'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "bureau_balance_data_grouped['STATUS_C_RATIO'] = bureau_balance_data_grouped['STATUS_C'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "bureau_balance_data_grouped['STATUS_0_RATIO'] = bureau_balance_data_grouped['STATUS_0'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "\n",
    "bureau_data = bureau_data.merge(bureau_balance_data_grouped, how = 'left')\n",
    "\n",
    "bureau_data[['MONTHS_BALANCE', 'STATUS_X', 'STATUS_C', 'BALANCE_COUNT', 'STATUS_0', 'STATUS_X_RATIO','STATUS_C_RATIO', 'STATUS_0_RATIO']] = bureau_data[['MONTHS_BALANCE', 'STATUS_X', 'STATUS_C', 'BALANCE_COUNT', 'STATUS_0', 'STATUS_X_RATIO','STATUS_C_RATIO', 'STATUS_0_RATIO']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bureau Data #####\n",
    "bureau_data_grouped = bureau_data.select_dtypes(exclude='object').drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR').sum()\n",
    "#bureau_data_grouped.columns = ['_'.join(col) if col != ('SK_ID_CURR', '') else col[0] for col in bureau_data_grouped.columns]\n",
    "bureau_data_grouped = bureau_data_grouped.reset_index()\n",
    "\n",
    "#Past Loan Count\n",
    "loan_count = bureau_data[['SK_ID_CURR','SK_ID_BUREAU']].groupby('SK_ID_CURR', \\\n",
    "                                                   as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU':'LOAN_COUNT'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(loan_count, how = 'left')\n",
    "\n",
    "#Unique Loan Types\n",
    "unique_loan_count = bureau_data[['SK_ID_CURR','CREDIT_TYPE']].groupby('SK_ID_CURR',\\\n",
    "                                                                      as_index=False).agg({'CREDIT_TYPE':'nunique'}).rename(columns={'CREDIT_TYPE':'UNIQUE_CREDIT_TYPES'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(unique_loan_count, how = 'left')\n",
    "\n",
    "#Total Active Loans\n",
    "bureau_data['CREDIT_ACTIVE_BINARY'] = bureau_data['CREDIT_ACTIVE'].apply(lambda x: 1 if x == 'Active' else 0)\n",
    "\n",
    "active_loan_count = bureau_data[['SK_ID_CURR','CREDIT_ACTIVE_BINARY']].groupby('SK_ID_CURR', \\\n",
    "                                                   as_index=False)['CREDIT_ACTIVE_BINARY'].sum().rename(columns = {'CREDIT_ACTIVE_BINARY':'ACTIVE_LOANS'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(active_loan_count, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Days Between Successive Past Applications\n",
    "grp = bureau_data[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])\n",
    "grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop = True)\n",
    "\n",
    "grp1['DAYS_CREDIT1'] = grp1['DAYS_CREDIT']*-1\n",
    "grp1['DAYS_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT1'].diff()\n",
    "grp1['DAYS_DIFF'] = grp1['DAYS_DIFF'].fillna(0).astype('uint32')\n",
    "del grp1['DAYS_CREDIT1'], grp1['DAYS_CREDIT']\n",
    "\n",
    "past_app_days = grp1.groupby('SK_ID_CURR', as_index=False)['DAYS_DIFF'].mean()\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(past_app_days, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days Credit Expires\n",
    "bureau_data['CREDIT_ENDDATE_BINARY'] = bureau_data['DAYS_CREDIT_ENDDATE'].apply(lambda x: 0 if x < 0 else 1) \n",
    "\n",
    "B1 = bureau_data.loc[bureau_data['CREDIT_ENDDATE_BINARY'] == 1]\n",
    "\n",
    "grp = B1[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT_ENDDATE']].groupby(by = ['SK_ID_CURR'])\n",
    "# Sort the values of CREDIT_ENDDATE for each customer ID \n",
    "grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT_ENDDATE'], ascending = True)).reset_index(drop = True)\n",
    "del grp\n",
    "\n",
    "grp1['DAYS_ENDDATE_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT_ENDDATE'].diff()\n",
    "grp1['DAYS_ENDDATE_DIFF'] = grp1['DAYS_ENDDATE_DIFF'].fillna(0).astype('uint32')\n",
    "del grp1['DAYS_CREDIT_ENDDATE']\n",
    "\n",
    "credit_expires_days = grp1.groupby('SK_ID_CURR', as_index = False)['DAYS_ENDDATE_DIFF'].mean()\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(credit_expires_days, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Active Loans\n",
    "bureau_data_grouped['ACTIVE_LOAN_PERC'] = bureau_data_grouped['ACTIVE_LOANS'] / bureau_data_grouped['LOAN_COUNT'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Credit Card Data #####\n",
    "cc_data_one_hot = pd.concat([cc_data['SK_ID_PREV'], \\\n",
    "                            pd.get_dummies(cc_data.select_dtypes(include=['object']), drop_first = True)],\\\n",
    "                            axis = 1)\n",
    "\n",
    "cc_data_one_hot = cc_data_one_hot.drop(['NAME_CONTRACT_STATUS_Approved','NAME_CONTRACT_STATUS_Demand','NAME_CONTRACT_STATUS_Demand',\\\n",
    "                      'NAME_CONTRACT_STATUS_Refused','NAME_CONTRACT_STATUS_Sent proposal','NAME_CONTRACT_STATUS_Signed'], axis = 1)\n",
    "\n",
    "cc_data_one_hot_grouped = cc_data_one_hot.groupby('SK_ID_PREV', as_index=False).sum()\n",
    "\n",
    "cc_data_numeric_grouped = cc_data.select_dtypes(exclude=['object']).groupby('SK_ID_PREV', as_index = False).agg(['count', sum, 'mean', min, max])\n",
    "\n",
    "cc_data_numeric_grouped.columns = ['_CC_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in cc_data_numeric_grouped.columns]\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', cc_data_numeric_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'MONTHS_BALANCE_CC_count')\n",
    "\n",
    "cc_data_numeric_grouped_filtered = cc_data_numeric_grouped[filtered_cols].reset_index()\n",
    "\n",
    "cc_data_grouped = cc_data_numeric_grouped_filtered.merge(cc_data_one_hot_grouped, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prev App Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app_data.columns = [x + '_PREV' for x in prev_app_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric Fields\n",
    "prev_app_data_num = prev_app_data.select_dtypes(exclude='object')\n",
    "prev_app_data_num = prev_app_data_num.drop(['RATE_INTEREST_PRIMARY_PREV','RATE_INTEREST_PRIVILEGED_PREV'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Fields\n",
    "prev_app_data_cat = prev_app_data.select_dtypes(include='object').fillna('Missing')\n",
    "\n",
    "prev_app_data_cat.NAME_CONTRACT_TYPE_PREV.replace('XNA_PREV', 'Revolving loans_PREV', inplace=True)\n",
    "prev_app_data_cat.NAME_CLIENT_TYPE_PREV.replace('XNA_PREV', 'Refreshed_PREV', inplace=True)\n",
    "prev_app_data_cat.NAME_PORTFOLIO_PREV.replace('Cars_PREV', 'Cards_PREV', inplace=True)\n",
    "\n",
    "prev_app_data_cat = prev_app_data_cat.drop(['FLAG_LAST_APPL_PER_CONTRACT_PREV'], axis = 1)\n",
    "\n",
    "prev_app_data_cat['NAME_CASH_LOAN_PURPOSE_PREV'] = prev_app_data_cat['NAME_CASH_LOAN_PURPOSE_PREV'].apply(lambda x: 'Other' if x != 'XAP' and x != 'XNA' else x)\n",
    "prev_app_data_cat['NAME_PAYMENT_TYPE_PREV'] = prev_app_data_cat['NAME_PAYMENT_TYPE_PREV'].apply(lambda x: 'Other' if x != 'Cash through the bank' and x != 'XNA' else x)\n",
    "prev_app_data_cat['NAME_GOODS_CATEGORY_PREV'] = prev_app_data_cat['NAME_GOODS_CATEGORY_PREV'].apply(lambda x: 'Other' if x not in ['XNA','Mobile',\\\n",
    "                                                                                                                         'Consumer Electronics','Computers','Audio/Video'] else x)\n",
    "prev_app_data_cat['NAME_SELLER_INDUSTRY_PREV'] = prev_app_data_cat['NAME_SELLER_INDUSTRY_PREV'].apply(lambda x: 'Other' if x not in ['XNA','Connectivity','Consumer Electronics'] else x)\n",
    "\n",
    "prev_app_data_cat = prev_app_data_cat.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "prev_app_data_clean = pd.concat([prev_app_data_num, prev_app_data_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped CC data and prev_app_data\n",
    "prev_app_data_merged = prev_app_data_clean.merge(cc_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_data['DAYS_ENTRY_PAYMENT'] = installments_data['DAYS_ENTRY_PAYMENT'].fillna(installments_data['DAYS_ENTRY_PAYMENT'].median())\n",
    "installments_data['AMT_PAYMENT'] = installments_data['AMT_PAYMENT'].fillna(installments_data['AMT_PAYMENT'].median())\n",
    "\n",
    "installments_data_grouped = installments_data.groupby('SK_ID_PREV', as_index = False).agg(['count', sum, 'mean', min, max])\n",
    "installments_data_grouped.columns = ['_INST_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in installments_data_grouped.columns]\n",
    "installments_data_grouped = installments_data_grouped.reset_index()\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', installments_data_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'SK_ID_CURR_INST_count')\n",
    "\n",
    "installments_data_grouped = installments_data_grouped[filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped installments and previous data\n",
    "prev_app_data_merged = prev_app_data_merged.merge(installments_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS CASH Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance_data['NAME_CONTRACT_STATUS'] = pos_cash_balance_data['NAME_CONTRACT_STATUS'].apply(lambda x: 'Other' if x not in ['Active','Completed','Signed'] else x)\n",
    "pos_cash_balance_data['NAME_CONTRACT_STATUS'] = LabelEncoder().fit_transform(pos_cash_balance_data['NAME_CONTRACT_STATUS'])\n",
    "\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data.groupby('SK_ID_PREV', as_index=False).agg(['count', sum, 'mean', min, max])\n",
    "pos_cash_balance_data_grouped.columns = ['_POS_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in pos_cash_balance_data_grouped.columns]\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data_grouped.reset_index()\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', pos_cash_balance_data_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'SK_ID_CURR_POS_count')\n",
    "\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data_grouped[filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped installments and previous data\n",
    "prev_app_data_merged = prev_app_data_merged.merge(pos_cash_balance_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bureau Data\n",
    "app_train_merged = train_clean.merge(bureau_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')\n",
    "app_test_merged = test_clean.merge(bureau_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prev App Data\n",
    "prev_app_data_grouped = prev_app_data_merged.drop('SK_ID_PREV', axis = 1).groupby('SK_ID_CURR', as_index=False).sum()\n",
    "\n",
    "app_train_merged = app_train_merged.merge(prev_app_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')\n",
    "app_test_merged = app_test_merged.merge(prev_app_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning final DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filling NA's with 0\n",
    "app_train_merged = app_train_merged.fillna(0).drop(['SK_ID_CURR','TARGET'], axis = 1)\n",
    "app_test_merged = app_test_merged.fillna(0).drop(['SK_ID_CURR'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree = 2)\n",
    "poly_transformer.fit(app_train_merged)\n",
    "train_poly_features = poly_transformer.transform(app_train_merged)\n",
    "\n",
    "train_subset_poly = pd.DataFrame(train_poly_features, columns = poly_transformer.get_feature_names(input_features = train_merged_imputed.columns.tolist()))\n",
    "\n",
    "test_poly_features = poly_transformer.transform(app_test_merged)\n",
    "test_subset_poly = pd.DataFrame(test_poly_features, columns = poly_transformer.get_feature_names(input_features = test_merged_imputed.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_poly_features)\n",
    "app_train_scaled = scaler.transform(train_poly_features)\n",
    "app_test_scaled = scaler.transform(app_test_merged.drop('SK_ID_CURR', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(iterations = 1000, random_state = 42, learning_rate = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_model.fit(app_train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(cat_model.predict_proba(app_test_scaled))\n",
    "submission_cat = pd.concat([test.SK_ID_CURR, preds], axis=1).drop(0, axis = 1)\n",
    "submission_cat.columns = ['SK_ID_CURR', 'Target']\n",
    "submission_cat.to_csv('cat_sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'SK_ID_CURR', u'TARGET', u'CNT_CHILDREN', u'AMT_INCOME_TOTAL',\n",
       "       u'AMT_CREDIT_x', u'AMT_ANNUITY_x', u'AMT_GOODS_PRICE_x',\n",
       "       u'REGION_POPULATION_RELATIVE', u'DAYS_BIRTH', u'DAYS_REGISTRATION',\n",
       "       ...\n",
       "       u'NAME_CONTRACT_STATUS_POS_min', u'NAME_CONTRACT_STATUS_POS_max',\n",
       "       u'SK_DPD_POS_sum', u'SK_DPD_POS_mean', u'SK_DPD_POS_min',\n",
       "       u'SK_DPD_POS_max', u'SK_DPD_DEF_POS_sum', u'SK_DPD_DEF_POS_mean',\n",
       "       u'SK_DPD_DEF_POS_min', u'SK_DPD_DEF_POS_max'],\n",
       "      dtype='object', length=308)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_subset = train_merged.dropna(thresh=len(train_merged) - 200000000, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_subset.info(verbose=True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_corr_subset = train_merged_subset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column_corr_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_corr = train_merged_subset.corr()['TARGET'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_corr_subset = col_corr[(col_corr >= 0.03) | (col_corr < -0.035)].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_corr_subset.remove('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_corr_subset.remove('CODE_GENDER_XNA')\n",
    "#column_corr_subset.remove('NAME_FAMILY_STATUS_Unknown')\n",
    "#column_corr_subset.remove('NAME_INCOME_TYPE_Maternity leave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_merged_subset[column_corr_subset]\n",
    "\n",
    "test_subset = test_merged[column_corr_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "imputer.fit(train_subset)\n",
    "train_merged_imputed = pd.DataFrame(imputer.transform(train_subset), columns = train_subset.columns)\n",
    "test_merged_imputed = pd.DataFrame(imputer.transform(test_subset), columns = train_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_imputed['DAYS_EMPLOYED_^2'] = train_merged_imputed['DAYS_EMPLOYED'] ** 2\n",
    "#train_merged_imputed['AMT_GOODS_PRICE_^2'] = train_merged_imputed['AMT_GOODS_PRICE'] ** 2\n",
    "train_merged_imputed['DAYS_CREDIT^2'] = train_merged_imputed['DAYS_CREDIT'] ** 2\n",
    "#train_merged_imputed['DAYS_CREDIT_median^2'] = train_merged_imputed['DAYS_CREDIT_median'] ** 2\n",
    "train_merged_imputed['DAYS_BIRTH_^2'] = train_merged_imputed['DAYS_BIRTH'] ** 2\n",
    "train_merged_imputed['REGION_RATING_CLIENT_W_CITY_^2'] = train_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 2\n",
    "train_merged_imputed['REGION_RATING_CLIENT_^2'] = train_merged_imputed['REGION_RATING_CLIENT'] ** 2\n",
    "train_merged_imputed['NAME_INCOME_TYPE_Working_^2'] = train_merged_imputed['NAME_INCOME_TYPE_Working'] ** 2\n",
    "train_merged_imputed['DAYS_LAST_PHONE_CHANGE_^2'] = train_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_1_^2'] = train_merged_imputed['EXT_SOURCE_1'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_2_^2'] = train_merged_imputed['EXT_SOURCE_2'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_3_^2'] = train_merged_imputed['EXT_SOURCE_3'] ** 2\n",
    "train_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^2'] = train_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 2\n",
    "train_merged_imputed['CODE_GENDER_F_^2']= train_merged_imputed['CODE_GENDER_F'] ** 2\n",
    "\n",
    "train_merged_imputed['DAYS_EMPLOYED_^3'] = train_merged_imputed['DAYS_EMPLOYED'] ** 3\n",
    "#train_merged_imputed['AMT_GOODS_PRICE_^3'] = train_merged_imputed['AMT_GOODS_PRICE'] ** 3\n",
    "train_merged_imputed['DAYS_CREDIT^3'] = train_merged_imputed['DAYS_CREDIT'] ** 3\n",
    "#train_merged_imputed['DAYS_CREDIT_median^3'] = train_merged_imputed['DAYS_CREDIT_median'] ** 3\n",
    "train_merged_imputed['DAYS_BIRTH_^3'] = train_merged_imputed['DAYS_BIRTH'] ** 3\n",
    "train_merged_imputed['REGION_RATING_CLIENT_W_CITY_^3'] = train_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 3\n",
    "train_merged_imputed['REGION_RATING_CLIENT_^3'] = train_merged_imputed['REGION_RATING_CLIENT'] ** 3\n",
    "train_merged_imputed['NAME_INCOME_TYPE_Working_^3'] = train_merged_imputed['NAME_INCOME_TYPE_Working'] ** 3\n",
    "train_merged_imputed['DAYS_LAST_PHONE_CHANGE_^3'] = train_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_1_^3'] = train_merged_imputed['EXT_SOURCE_1'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_2_^3'] = train_merged_imputed['EXT_SOURCE_2'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_3_^3'] = train_merged_imputed['EXT_SOURCE_3'] ** 3\n",
    "train_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^3'] = train_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 3\n",
    "train_merged_imputed['CODE_GENDER_F_^3']= train_merged_imputed['CODE_GENDER_F'] ** 3\n",
    "\n",
    "test_merged_imputed['DAYS_EMPLOYED_^2'] = test_merged_imputed['DAYS_EMPLOYED'] ** 2\n",
    "#test_merged_imputed['AMT_GOODS_PRICE_^2'] = test_merged_imputed['AMT_GOODS_PRICE'] ** 2\n",
    "test_merged_imputed['DAYS_CREDIT_^2'] = test_merged_imputed['DAYS_CREDIT'] ** 2\n",
    "#test_merged_imputed['DAYS_CREDIT_median^2'] = test_merged_imputed['DAYS_CREDIT_median'] ** 2\n",
    "test_merged_imputed['DAYS_BIRTH_^2'] = test_merged_imputed['DAYS_BIRTH'] ** 2\n",
    "test_merged_imputed['REGION_RATING_CLIENT_W_CITY_^2'] = test_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 2\n",
    "test_merged_imputed['REGION_RATING_CLIENT_^2'] = test_merged_imputed['REGION_RATING_CLIENT'] ** 2\n",
    "test_merged_imputed['NAME_INCOME_TYPE_Working_^2'] = test_merged_imputed['NAME_INCOME_TYPE_Working'] ** 2\n",
    "test_merged_imputed['DAYS_LAST_PHONE_CHANGE_^2'] = test_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_1_^2'] = test_merged_imputed['EXT_SOURCE_1'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_2_^2'] = test_merged_imputed['EXT_SOURCE_2'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_3_^2'] = test_merged_imputed['EXT_SOURCE_3'] ** 2\n",
    "test_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^2'] = test_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 2\n",
    "test_merged_imputed['CODE_GENDER_F_^2']= test_merged_imputed['CODE_GENDER_F'] ** 2\n",
    "\n",
    "test_merged_imputed['DAYS_EMPLOYED_^3'] = test_merged_imputed['DAYS_EMPLOYED'] ** 3\n",
    "#test_merged_imputed['AMT_GOODS_PRICE_^3'] = test_merged_imputed['AMT_GOODS_PRICE'] ** 3\n",
    "test_merged_imputed['DAYS_CREDIT^3'] = test_merged_imputed['DAYS_CREDIT'] ** 3\n",
    "#test_merged_imputed['DAYS_CREDIT_median^3'] = test_merged_imputed['DAYS_CREDIT_median'] ** 3\n",
    "test_merged_imputed['DAYS_BIRTH_^3'] = test_merged_imputed['DAYS_BIRTH'] ** 3\n",
    "test_merged_imputed['REGION_RATING_CLIENT_W_CITY_^3'] = test_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 3\n",
    "test_merged_imputed['REGION_RATING_CLIENT_^3'] = test_merged_imputed['REGION_RATING_CLIENT'] ** 3\n",
    "test_merged_imputed['NAME_INCOME_TYPE_Working_^3'] = test_merged_imputed['NAME_INCOME_TYPE_Working'] ** 3\n",
    "test_merged_imputed['DAYS_LAST_PHONE_CHANGE_^3'] = test_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_1_^3'] = test_merged_imputed['EXT_SOURCE_1'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_2_^3'] = test_merged_imputed['EXT_SOURCE_2'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_3_^3'] = test_merged_imputed['EXT_SOURCE_3'] ** 3\n",
    "test_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^3'] = test_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 3\n",
    "test_merged_imputed['CODE_GENDER_F_^3']= test_merged_imputed['CODE_GENDER_F'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree = 1)\n",
    "poly_transformer.fit(train_merged_imputed)\n",
    "train_poly_features = poly_transformer.transform(train_merged_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_poly = pd.DataFrame(train_poly_features, columns = poly_transformer.get_feature_names(\n",
    "    input_features = train_merged_imputed.columns.tolist()\n",
    "))\n",
    "\n",
    "test_poly_features = poly_transformer.transform(test_merged_imputed)\n",
    "test_subset_poly = pd.DataFrame(test_poly_features, columns = poly_transformer.get_feature_names(input_features = test_merged_imputed.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_subset_poly)\n",
    "train_scaled = scaler.transform(train_subset_poly)\n",
    "test_scaled = scaler.transform(test_subset_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(iterations = 1000, random_state = 42, learning_rate = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_model.fit(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#xgb_model = XGBClassifier(n_estimators = 500, silent=True, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model.fit(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_cat = pd.DataFrame(xgb_model.predict_proba(test_scaled))\n",
    "submission_cat = pd.concat([test.SK_ID_CURR, test_y_cat], axis=1).drop(0, axis = 1)\n",
    "submission_cat.columns = ['SK_ID_CURR', 'Target']\n",
    "submission_cat.to_csv('xgb_model10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_cat['Target'] = submission_cat['Target'].apply(lambda x: (x - min_sub_cat) / (max_sub_cat - min_sub_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_submission = pd.read_csv('cat_lr75.csv')\n",
    "xgb_submission = pd.read_csv('xgb1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat['Target'] = (submission_cat['Target']+ xgb_submission['Target']) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.to_csv('combined11.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lightgbm.Dataset(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "#                       valid_sets=test_data,\n",
    "                       num_boost_round=10000,\n",
    "#                       early_stopping_rounds=100\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(model.predict(test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbm = pd.concat([test.SK_ID_CURR, preds], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbm.columns = ['SK_ID_CURR', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.to_csv('lgbm2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10 = pd.read_csv('combined10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10['Target'] = (combined_10['Target'] + submission_cat['Target']) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10.to_csv('new_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
