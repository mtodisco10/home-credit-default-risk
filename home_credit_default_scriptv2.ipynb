{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Data\n",
    "train = pd.read_csv('dataFiles/application_train.csv')\n",
    "test = pd.read_csv('dataFiles/application_test.csv')\n",
    "bureau_data = pd.read_csv('dataFiles/bureau.csv')\n",
    "bureau_balance_data = pd.read_csv('dataFiles/bureau_balance.csv')\n",
    "prev_app_data = pd.read_csv('dataFiles/previous_application.csv')\n",
    "pos_cash_balance_data = pd.read_csv('dataFiles/POS_CASH_balance.csv')\n",
    "installments_data = pd.read_csv('dataFiles/installments_payments.csv')\n",
    "cc_data = pd.read_csv('dataFiles/credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.select_dtypes(exclude='object')\n",
    "test_num = test.select_dtypes(exclude='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transformations\n",
    "train_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',\\\n",
    "       'AMT_GOODS_PRICE']] = np.log(train_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']])\n",
    "\n",
    "test_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY',\\\n",
    "       'AMT_GOODS_PRICE']] = np.log(test_num[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clipping Outliers\n",
    "train_num['CNT_CHILDREN'] = train_num['CNT_CHILDREN'].apply(lambda x: 5 if x >=5 else x)\n",
    "train_num['CNT_FAM_MEMBERS'] = train_num['CNT_FAM_MEMBERS'].apply(lambda x: 5 if x >=5 else x)\n",
    "train_num['AMT_INCOME_TOTAL'] = train_num['AMT_INCOME_TOTAL'].apply(lambda x: 14 if x >=14 else x)\n",
    "train_num['HOUR_APPR_PROCESS_START'] = train_num['HOUR_APPR_PROCESS_START'].apply(lambda x: 2 if x <= 2 else x)\n",
    "train_num['COMMONAREA_AVG'] = train_num['COMMONAREA_AVG'].fillna(0).apply(lambda x: 0.3 if x >= 0.3 else x)\n",
    "train_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = train_num['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda x: 9 if x >= 9 else x).fillna(10)\n",
    "\n",
    "test_num['CNT_CHILDREN'] = test_num['CNT_CHILDREN'].apply(lambda x: 5 if x >=5 else x)\n",
    "test_num['CNT_FAM_MEMBERS'] = test_num['CNT_FAM_MEMBERS'].apply(lambda x: 5 if x >=5 else x)\n",
    "test_num['AMT_INCOME_TOTAL'] = test_num['AMT_INCOME_TOTAL'].apply(lambda x: 14 if x >=14 else x)\n",
    "test_num['HOUR_APPR_PROCESS_START'] = test_num['HOUR_APPR_PROCESS_START'].apply(lambda x: 2 if x <= 2 else x)\n",
    "test_num['COMMONAREA_AVG'] = test_num['COMMONAREA_AVG'].fillna(0).apply(lambda x: 0.3 if x >= 0.3 else x)\n",
    "test_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = test_num['AMT_REQ_CREDIT_BUREAU_YEAR'].apply(lambda x: 9 if x >= 9 else x).fillna(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINNING THE TWO HUMPS FROM DAYS_EMPLOYED\n",
    "train_num['DAYS_EMPLOYED_BIN_1'] = train_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x < 150000 else 0)\n",
    "train_num['DAYS_EMPLOYED_BIN_2'] = train_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x >= 150000 else 0)\n",
    "\n",
    "#BINNING THE TWO HUMPS FROM DAYS_EMPLOYED\n",
    "test_num['DAYS_EMPLOYED_BIN_1'] = test_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x < 150000 else 0)\n",
    "test_num['DAYS_EMPLOYED_BIN_2'] = test_num['DAYS_EMPLOYED'].apply(lambda x: 1 if x >= 150000 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABS & BOX COX Transformation\n",
    "train_num['DAYS_REGISTRATION'] = np.abs(train_num['DAYS_REGISTRATION'])\n",
    "train_num['DAYS_REGISTRATION'] = train_num['DAYS_REGISTRATION'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "train_num['DAYS_REGISTRATION'] = boxcox(train_num['DAYS_REGISTRATION'],0.5)\n",
    "\n",
    "train_num['DAYS_ID_PUBLISH'] = np.abs(train_num['DAYS_ID_PUBLISH'])\n",
    "train_num['DAYS_ID_PUBLISH'] = train_num['DAYS_ID_PUBLISH'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "train_num['DAYS_ID_PUBLISH'] = boxcox(train_num['DAYS_ID_PUBLISH'],0.5)\n",
    "\n",
    "train_num['ENTRANCES_AVG'] = train_num['ENTRANCES_AVG'].apply(lambda x: np.sqrt(x) if pd.notnull(x) else x)\n",
    "\n",
    "test_num['DAYS_REGISTRATION'] = np.abs(test_num['DAYS_REGISTRATION'])\n",
    "test_num['DAYS_REGISTRATION'] = test_num['DAYS_REGISTRATION'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "test_num['DAYS_REGISTRATION'] = boxcox(test_num['DAYS_REGISTRATION'],0.5)\n",
    "\n",
    "test_num['DAYS_ID_PUBLISH'] = np.abs(test_num['DAYS_ID_PUBLISH'])\n",
    "test_num['DAYS_ID_PUBLISH'] = test_num['DAYS_ID_PUBLISH'].apply(lambda x: 0.01 if x == 0 else x)\n",
    "test_num['DAYS_ID_PUBLISH'] = boxcox(test_num['DAYS_ID_PUBLISH'],0.5)\n",
    "\n",
    "test_num['ENTRANCES_AVG'] = test_num['ENTRANCES_AVG'].apply(lambda x: np.sqrt(x) if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Bins for Own Car Age\n",
    "def own_car_age_bins(x):\n",
    "    if pd.isnull(x):\n",
    "        return 0\n",
    "    elif x <= 10:\n",
    "        return 1\n",
    "    elif x <= 30:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "train['OWN_CAR_AGE_BINS'] = train['OWN_CAR_AGE'].apply(own_car_age_bins)\n",
    "\n",
    "test['OWN_CAR_AGE_BINS'] = test['OWN_CAR_AGE'].apply(own_car_age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to drop because of sparse features\n",
    "train_num = train_num.drop(['FLAG_MOBIL','FLAG_CONT_MOBILE','FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\\\n",
    "                            'FLAG_DOCUMENT_7','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_15',\\\n",
    "                            'FLAG_DOCUMENT_17','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\\\n",
    "                            'OWN_CAR_AGE','DAYS_EMPLOYED','DAYS_EMPLOYED_BIN_1','AMT_REQ_CREDIT_BUREAU_HOUR',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_QRT'], axis = 1)\n",
    "\n",
    "test_num = test_num.drop(['FLAG_MOBIL','FLAG_CONT_MOBILE','FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\\\n",
    "                          'FLAG_DOCUMENT_7','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12','FLAG_DOCUMENT_15',\\\n",
    "                          'FLAG_DOCUMENT_17','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21',\\\n",
    "                          'OWN_CAR_AGE','DAYS_EMPLOYED','DAYS_EMPLOYED_BIN_1','AMT_REQ_CREDIT_BUREAU_HOUR',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON',\\\n",
    "                            'AMT_REQ_CREDIT_BUREAU_QRT'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FillNA Columns\n",
    "train_num.iloc[:,25:76] = train_num.iloc[:,25:76].fillna(0).astype(int)\n",
    "train_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = train_num['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(11).astype(int)\n",
    "\n",
    "test_num.iloc[:,24:75] = test_num.iloc[:,24:75].fillna(0).astype(int)\n",
    "test_num['AMT_REQ_CREDIT_BUREAU_YEAR'] = test_num['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(11).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Median Columns\n",
    "train_num.loc[:,'AMT_ANNUITY'] = train_num.loc[:,'AMT_ANNUITY'].fillna(train_num.AMT_ANNUITY.median())\n",
    "train_num.loc[:,'AMT_GOODS_PRICE'] = train_num.loc[:,'AMT_GOODS_PRICE'].fillna(train_num.AMT_GOODS_PRICE.median())\n",
    "train_num.loc[:,'CNT_FAM_MEMBERS'] = train_num.loc[:,'CNT_FAM_MEMBERS'].fillna(train_num.CNT_FAM_MEMBERS.median())\n",
    "\n",
    "test_num.loc[:,'AMT_ANNUITY'] = test_num.loc[:,'AMT_ANNUITY'].fillna(test_num.AMT_ANNUITY.median())\n",
    "test_num.loc[:,'AMT_GOODS_PRICE'] = test_num.loc[:,'AMT_GOODS_PRICE'].fillna(test_num.AMT_GOODS_PRICE.median())\n",
    "test_num.loc[:,'CNT_FAM_MEMBERS'] = test_num.loc[:,'CNT_FAM_MEMBERS'].fillna(test_num.CNT_FAM_MEMBERS.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MikeTodisco/anaconda2/lib/python2.7/site-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_num['children_ratio'] = train_num['CNT_CHILDREN'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['credit_to_annuity_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_ANNUITY']\n",
    "train_num['credit_to_goods_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_GOODS_PRICE']\n",
    "train_num['credit_to_income_ratio'] = train_num['AMT_CREDIT'] / train_num['AMT_INCOME_TOTAL']\n",
    "train_num['income_credit_percentage'] = train_num['AMT_INCOME_TOTAL'] / train_num['AMT_CREDIT']\n",
    "train_num['income_per_child'] = train_num['AMT_INCOME_TOTAL'] / (1 + train_num['CNT_CHILDREN'])\n",
    "train_num['income_per_person'] = train_num['AMT_INCOME_TOTAL'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['payment_rate'] = train_num['AMT_ANNUITY'] / train_num['AMT_CREDIT']\n",
    "train_num['phone_to_birth_ratio'] = train_num['DAYS_LAST_PHONE_CHANGE'] / train_num['DAYS_BIRTH']\n",
    "train_num['cnt_non_child'] = train_num['CNT_FAM_MEMBERS'] - train_num['CNT_CHILDREN']\n",
    "train_num['child_to_non_child_ratio'] = train_num['CNT_CHILDREN'] / (1+ train_num['cnt_non_child'])\n",
    "train_num['income_per_non_child'] = train_num['AMT_INCOME_TOTAL'] / (1 + train_num['cnt_non_child'])\n",
    "train_num['credit_per_person'] = train_num['AMT_CREDIT'] / train_num['CNT_FAM_MEMBERS']\n",
    "train_num['credit_per_child'] = train_num['AMT_CREDIT'] / (1 + train_num['CNT_CHILDREN'])\n",
    "train_num['credit_per_non_child'] = train_num['AMT_CREDIT'] / (1 + train_num['cnt_non_child'])\n",
    "\n",
    "test_num['children_ratio'] = test_num['CNT_CHILDREN'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['credit_to_annuity_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_ANNUITY']\n",
    "test_num['credit_to_goods_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_GOODS_PRICE']\n",
    "test_num['credit_to_income_ratio'] = test_num['AMT_CREDIT'] / test_num['AMT_INCOME_TOTAL']\n",
    "test_num['income_credit_percentage'] = test_num['AMT_INCOME_TOTAL'] / test_num['AMT_CREDIT']\n",
    "test_num['income_per_child'] = test_num['AMT_INCOME_TOTAL'] / (1 + test_num['CNT_CHILDREN'])\n",
    "test_num['income_per_person'] = test_num['AMT_INCOME_TOTAL'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['payment_rate'] = test_num['AMT_ANNUITY'] / test_num['AMT_CREDIT']\n",
    "test_num['phone_to_birth_ratio'] = test_num['DAYS_LAST_PHONE_CHANGE'] / test_num['DAYS_BIRTH']\n",
    "test_num['cnt_non_child'] = test_num['CNT_FAM_MEMBERS'] - test_num['CNT_CHILDREN']\n",
    "test_num['child_to_non_child_ratio'] = test_num['CNT_CHILDREN'] / (1 + test_num['cnt_non_child'])\n",
    "test_num['income_per_non_child'] = test_num['AMT_INCOME_TOTAL'] / (1 + test_num['cnt_non_child'])\n",
    "test_num['credit_per_person'] = test_num['AMT_CREDIT'] / test_num['CNT_FAM_MEMBERS']\n",
    "test_num['credit_per_child'] = test_num['AMT_CREDIT'] / (1 + test_num['CNT_CHILDREN'])\n",
    "test_num['credit_per_non_child'] = test_num['AMT_CREDIT'] / (1 + test_num['cnt_non_child']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Categorical Variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_cat = train.select_dtypes(include='object').fillna('Missing')\n",
    "test_cat = test.select_dtypes(include='object').fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that are sparse\n",
    "train_cat = train_cat.drop('ORGANIZATION_TYPE', axis = 1)\n",
    "test_cat = test_cat.drop('ORGANIZATION_TYPE', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "train_cat = train_cat.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "test_cat = test_cat.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating Train & Test Numerical & Categorical Features\n",
    "train_clean = pd.concat([train_num, train_cat], axis = 1)\n",
    "\n",
    "test_clean = pd.concat([test_num, test_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_sample(train_num_scaled, train.TARGET)\n",
    "\n",
    "# from collections import Counter\n",
    "# print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bureau Balance ####\n",
    "bureau_balance_data_grouped = pd.get_dummies(bureau_balance_data).groupby('SK_ID_BUREAU', as_index=False).agg({'STATUS_1':'count','MONTHS_BALANCE':min,\\\n",
    "                                                                                 'STATUS_C':sum,'STATUS_0':sum,'STATUS_X':sum})\n",
    "\n",
    "bureau_balance_data_grouped = bureau_balance_data_grouped.rename(columns={'STATUS_1':'BALANCE_COUNT'})\n",
    "\n",
    "bureau_balance_data_grouped['STATUS_X_RATIO'] = bureau_balance_data_grouped['STATUS_X'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "bureau_balance_data_grouped['STATUS_C_RATIO'] = bureau_balance_data_grouped['STATUS_C'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "bureau_balance_data_grouped['STATUS_0_RATIO'] = bureau_balance_data_grouped['STATUS_0'] / bureau_balance_data_grouped['BALANCE_COUNT'].astype(float)\n",
    "\n",
    "bureau_data = bureau_data.merge(bureau_balance_data_grouped, how = 'left')\n",
    "\n",
    "bureau_data[['MONTHS_BALANCE', 'STATUS_X', 'STATUS_C', 'BALANCE_COUNT', 'STATUS_0', 'STATUS_X_RATIO','STATUS_C_RATIO', 'STATUS_0_RATIO']] = bureau_data[['MONTHS_BALANCE', 'STATUS_X', 'STATUS_C', 'BALANCE_COUNT', 'STATUS_0', 'STATUS_X_RATIO','STATUS_C_RATIO', 'STATUS_0_RATIO']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bureau Data #####\n",
    "bureau_data_grouped = bureau_data.select_dtypes(exclude='object').drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR').sum()\n",
    "#bureau_data_grouped.columns = ['_'.join(col) if col != ('SK_ID_CURR', '') else col[0] for col in bureau_data_grouped.columns]\n",
    "bureau_data_grouped = bureau_data_grouped.reset_index()\n",
    "\n",
    "#Past Loan Count\n",
    "loan_count = bureau_data[['SK_ID_CURR','SK_ID_BUREAU']].groupby('SK_ID_CURR', \\\n",
    "                                                   as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU':'LOAN_COUNT'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(loan_count, how = 'left')\n",
    "\n",
    "#Unique Loan Types\n",
    "unique_loan_count = bureau_data[['SK_ID_CURR','CREDIT_TYPE']].groupby('SK_ID_CURR',\\\n",
    "                                                                      as_index=False).agg({'CREDIT_TYPE':'nunique'}).rename(columns={'CREDIT_TYPE':'UNIQUE_CREDIT_TYPES'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(unique_loan_count, how = 'left')\n",
    "\n",
    "#Total Active Loans\n",
    "bureau_data['CREDIT_ACTIVE_BINARY'] = bureau_data['CREDIT_ACTIVE'].apply(lambda x: 1 if x == 'Active' else 0)\n",
    "\n",
    "active_loan_count = bureau_data[['SK_ID_CURR','CREDIT_ACTIVE_BINARY']].groupby('SK_ID_CURR', \\\n",
    "                                                   as_index=False)['CREDIT_ACTIVE_BINARY'].sum().rename(columns = {'CREDIT_ACTIVE_BINARY':'ACTIVE_LOANS'})\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(active_loan_count, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Days Between Successive Past Applications\n",
    "grp = bureau_data[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])\n",
    "grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop = True)\n",
    "\n",
    "grp1['DAYS_CREDIT1'] = grp1['DAYS_CREDIT']*-1\n",
    "grp1['DAYS_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT1'].diff()\n",
    "grp1['DAYS_DIFF'] = grp1['DAYS_DIFF'].fillna(0).astype('uint32')\n",
    "del grp1['DAYS_CREDIT1'], grp1['DAYS_CREDIT']\n",
    "\n",
    "past_app_days = grp1.groupby('SK_ID_CURR', as_index=False)['DAYS_DIFF'].mean()\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(past_app_days, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days Credit Expires\n",
    "bureau_data['CREDIT_ENDDATE_BINARY'] = bureau_data['DAYS_CREDIT_ENDDATE'].apply(lambda x: 0 if x < 0 else 1) \n",
    "\n",
    "B1 = bureau_data.loc[bureau_data['CREDIT_ENDDATE_BINARY'] == 1]\n",
    "\n",
    "grp = B1[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT_ENDDATE']].groupby(by = ['SK_ID_CURR'])\n",
    "# Sort the values of CREDIT_ENDDATE for each customer ID \n",
    "grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT_ENDDATE'], ascending = True)).reset_index(drop = True)\n",
    "del grp\n",
    "\n",
    "grp1['DAYS_ENDDATE_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT_ENDDATE'].diff()\n",
    "grp1['DAYS_ENDDATE_DIFF'] = grp1['DAYS_ENDDATE_DIFF'].fillna(0).astype('uint32')\n",
    "del grp1['DAYS_CREDIT_ENDDATE']\n",
    "\n",
    "credit_expires_days = grp1.groupby('SK_ID_CURR', as_index = False)['DAYS_ENDDATE_DIFF'].mean()\n",
    "\n",
    "bureau_data_grouped = bureau_data_grouped.merge(credit_expires_days, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Active Loans\n",
    "bureau_data_grouped['ACTIVE_LOAN_PERC'] = bureau_data_grouped['ACTIVE_LOANS'] / bureau_data_grouped['LOAN_COUNT'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loans per Customer\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index().rename(index = str, columns = {'SK_ID_PREV': 'NO_LOANS'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Installments paid by customer per loan\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV'])['CNT_INSTALMENT_MATURE_CUM'].max().reset_index().rename(index = str, columns = {'CNT_INSTALMENT_MATURE_CUM': 'NO_INSTALMENTS'})\n",
    "grp = grp.groupby(by = ['SK_ID_CURR'])['NO_INSTALMENTS'].sum().reset_index().rename(index = str, columns = {'NO_INSTALMENTS': 'TOTAL_INSTALMENTS'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg Number of Installments paid per Loan\n",
    "cc_data['INSTALLMENTS_PER_LOAN'] = (cc_data['TOTAL_INSTALMENTS']/cc_data['NO_LOANS']).astype('uint32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg % Loading of Credit Limit Per Customer\n",
    "cc_data['AMT_CREDIT_LIMIT_ACTUAL1'] = cc_data['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "def f(x1, x2):\n",
    "    \n",
    "    balance = x1.max() + 1\n",
    "    limit = x2.max() + 1\n",
    "    \n",
    "    return (balance/limit)\n",
    "\n",
    "# Calculate the ratio of Amount Balance to Credit Limit - CREDIT LOAD OF CUSTOMER \n",
    "# This is done for each Credit limit value per loan per Customer \n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV', 'AMT_CREDIT_LIMIT_ACTUAL']).apply(lambda x: f(x.AMT_BALANCE, x.AMT_CREDIT_LIMIT_ACTUAL1)).reset_index().rename(index = str, columns = {0: 'CREDIT_LOAD1'})\n",
    "del cc_data['AMT_CREDIT_LIMIT_ACTUAL1']\n",
    "\n",
    "# We now calculate the mean Credit load of All Loan transactions of Customer \n",
    "grp1 = grp.groupby(by = ['SK_ID_CURR'])['CREDIT_LOAD1'].mean().reset_index().rename(index = str, columns = {'CREDIT_LOAD1': 'CREDIT_LOAD'})\n",
    "\n",
    "cc_data = cc_data.merge(grp1, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of times Days Past Due occurred \n",
    "\n",
    "def f(DPD):\n",
    "    \n",
    "    # DPD is a series of values of SK_DPD for each of the groupby combination \n",
    "    # We convert it to a list to get the number of SK_DPD values NOT EQUALS ZERO\n",
    "    x = DPD.tolist()\n",
    "    c = 0\n",
    "    for i,j in enumerate(x):\n",
    "        if j != 0:\n",
    "            c += 1\n",
    "    \n",
    "    return c \n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR', 'SK_ID_PREV']).apply(lambda x: f(x.SK_DPD)).reset_index().rename(index = str, columns = {0: 'NO_DPD'})\n",
    "grp1 = grp.groupby(by = ['SK_ID_CURR'])['NO_DPD'].mean().reset_index().rename(index = str, columns = {'NO_DPD' : 'DPD_COUNT'})\n",
    "\n",
    "cc_data = cc_data.merge(grp1, on = ['SK_ID_CURR'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average of Days Past Due Per Customer\n",
    "grp = cc_data.groupby(by= ['SK_ID_CURR'])['SK_DPD'].mean().reset_index().rename(index = str, columns = {'SK_DPD': 'AVG_DPD'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of Minimum Payments Missed\n",
    "def f(min_pay, total_pay):\n",
    "    \n",
    "    M = min_pay.tolist()\n",
    "    T = total_pay.tolist()\n",
    "    P = len(M)\n",
    "    c = 0 \n",
    "    # Find the count of transactions when Payment made is less than Minimum Payment \n",
    "    for i in range(len(M)):\n",
    "        if T[i] < M[i]:\n",
    "            c += 1  \n",
    "    return (100*c)/P\n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR']).apply(lambda x: f(x.AMT_INST_MIN_REGULARITY, x.AMT_PAYMENT_CURRENT)).reset_index().rename(index = str, columns = { 0 : 'PERCENTAGE_MISSED_PAYMENTS'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of Cash vs Card Swipes\n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_ATM_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_ATM_CURRENT' : 'DRAWINGS_ATM'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_CURRENT' : 'DRAWINGS_TOTAL'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "cc_data['CASH_CARD_RATIO1'] = (cc_data['DRAWINGS_ATM']/cc_data['DRAWINGS_TOTAL'])*100\n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['CASH_CARD_RATIO1'].mean().reset_index().rename(index = str, columns ={ 'CASH_CARD_RATIO1' : 'CASH_CARD_RATIO'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg Drawing per Customer\n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['AMT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'AMT_DRAWINGS_CURRENT' : 'TOTAL_DRAWINGS'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['CNT_DRAWINGS_CURRENT'].sum().reset_index().rename(index = str, columns = {'CNT_DRAWINGS_CURRENT' : 'NO_DRAWINGS'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "cc_data['DRAWINGS_RATIO1'] = (cc_data['TOTAL_DRAWINGS']/cc_data['NO_DRAWINGS'])*100\n",
    "\n",
    "grp = cc_data.groupby(by = ['SK_ID_CURR'])['DRAWINGS_RATIO1'].mean().reset_index().rename(index = str, columns ={ 'DRAWINGS_RATIO1' : 'DRAWINGS_RATIO'})\n",
    "cc_data = cc_data.merge(grp, on = ['SK_ID_CURR'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Credit Card Data #####\n",
    "cc_data_one_hot = pd.concat([cc_data['SK_ID_PREV'], \\\n",
    "                            pd.get_dummies(cc_data.select_dtypes(include=['object']), drop_first = True)],\\\n",
    "                            axis = 1)\n",
    "\n",
    "cc_data_one_hot = cc_data_one_hot.drop(['NAME_CONTRACT_STATUS_Approved','NAME_CONTRACT_STATUS_Demand','NAME_CONTRACT_STATUS_Demand',\\\n",
    "                      'NAME_CONTRACT_STATUS_Refused','NAME_CONTRACT_STATUS_Sent proposal','NAME_CONTRACT_STATUS_Signed'], axis = 1)\n",
    "\n",
    "cc_data_one_hot_grouped = cc_data_one_hot.groupby('SK_ID_PREV', as_index=False).sum()\n",
    "\n",
    "cc_data_numeric_grouped = cc_data.select_dtypes(exclude=['object']).groupby('SK_ID_PREV', as_index = False).agg(['count', sum, 'mean', min, max])\n",
    "\n",
    "cc_data_numeric_grouped.columns = ['_CC_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in cc_data_numeric_grouped.columns]\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', cc_data_numeric_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'MONTHS_BALANCE_CC_count')\n",
    "\n",
    "cc_data_numeric_grouped_filtered = cc_data_numeric_grouped[filtered_cols].reset_index()\n",
    "\n",
    "cc_data_grouped = cc_data_numeric_grouped_filtered.merge(cc_data_one_hot_grouped, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prev App Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app_data.columns = [x + '_PREV' if x not in ('SK_ID_PREV','SK_ID_CURR') else x for x in prev_app_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numeric Fields\n",
    "prev_app_data_num = prev_app_data.select_dtypes(exclude='object')\n",
    "prev_app_data_num = prev_app_data_num.drop(['RATE_INTEREST_PRIMARY_PREV','RATE_INTEREST_PRIVILEGED_PREV'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Fields\n",
    "prev_app_data_cat = prev_app_data.select_dtypes(include='object').fillna('Missing')\n",
    "\n",
    "prev_app_data_cat.NAME_CONTRACT_TYPE_PREV.replace('XNA_PREV', 'Revolving loans_PREV', inplace=True)\n",
    "prev_app_data_cat.NAME_CLIENT_TYPE_PREV.replace('XNA_PREV', 'Refreshed_PREV', inplace=True)\n",
    "prev_app_data_cat.NAME_PORTFOLIO_PREV.replace('Cars_PREV', 'Cards_PREV', inplace=True)\n",
    "\n",
    "prev_app_data_cat = prev_app_data_cat.drop(['FLAG_LAST_APPL_PER_CONTRACT_PREV'], axis = 1)\n",
    "\n",
    "prev_app_data_cat['NAME_CASH_LOAN_PURPOSE_PREV'] = prev_app_data_cat['NAME_CASH_LOAN_PURPOSE_PREV'].apply(lambda x: 'Other' if x != 'XAP' and x != 'XNA' else x)\n",
    "prev_app_data_cat['NAME_PAYMENT_TYPE_PREV'] = prev_app_data_cat['NAME_PAYMENT_TYPE_PREV'].apply(lambda x: 'Other' if x != 'Cash through the bank' and x != 'XNA' else x)\n",
    "prev_app_data_cat['NAME_GOODS_CATEGORY_PREV'] = prev_app_data_cat['NAME_GOODS_CATEGORY_PREV'].apply(lambda x: 'Other' if x not in ['XNA','Mobile',\\\n",
    "                                                                                                                         'Consumer Electronics','Computers','Audio/Video'] else x)\n",
    "prev_app_data_cat['NAME_SELLER_INDUSTRY_PREV'] = prev_app_data_cat['NAME_SELLER_INDUSTRY_PREV'].apply(lambda x: 'Other' if x not in ['XNA','Connectivity','Consumer Electronics'] else x)\n",
    "\n",
    "prev_app_data_cat = prev_app_data_cat.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "prev_app_data_clean = pd.concat([prev_app_data_num, prev_app_data_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped CC data and prev_app_data\n",
    "prev_app_data_merged = prev_app_data_clean.merge(cc_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_data['DAYS_ENTRY_PAYMENT'] = installments_data['DAYS_ENTRY_PAYMENT'].fillna(installments_data['DAYS_ENTRY_PAYMENT'].median())\n",
    "installments_data['AMT_PAYMENT'] = installments_data['AMT_PAYMENT'].fillna(installments_data['AMT_PAYMENT'].median())\n",
    "\n",
    "installments_data_grouped = installments_data.groupby('SK_ID_PREV', as_index = False).agg(['count', sum, 'mean', min, max])\n",
    "installments_data_grouped.columns = ['_INST_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in installments_data_grouped.columns]\n",
    "installments_data_grouped = installments_data_grouped.reset_index()\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', installments_data_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'SK_ID_CURR_INST_count')\n",
    "\n",
    "installments_data_grouped = installments_data_grouped[filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped installments and previous data\n",
    "prev_app_data_merged = prev_app_data_merged.merge(installments_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS CASH Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance_data['NAME_CONTRACT_STATUS'] = pos_cash_balance_data['NAME_CONTRACT_STATUS'].apply(lambda x: 'Other' if x not in ['Active','Completed','Signed'] else x)\n",
    "pos_cash_balance_data['NAME_CONTRACT_STATUS'] = LabelEncoder().fit_transform(pos_cash_balance_data['NAME_CONTRACT_STATUS'])\n",
    "\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data.groupby('SK_ID_PREV', as_index=False).agg(['count', sum, 'mean', min, max])\n",
    "pos_cash_balance_data_grouped.columns = ['_POS_'.join(col) if col != ('SK_ID_PREV', '') else col[0] for col in pos_cash_balance_data_grouped.columns]\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data_grouped.reset_index()\n",
    "\n",
    "filtered_cols = filter(lambda x: x[-5:] != 'count' and x[0:10] != 'SK_ID_CURR', pos_cash_balance_data_grouped.columns.tolist())\n",
    "filtered_cols.insert(0, 'SK_ID_CURR_POS_count')\n",
    "\n",
    "pos_cash_balance_data_grouped = pos_cash_balance_data_grouped[filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging grouped installments and previous data\n",
    "prev_app_data_merged = prev_app_data_merged.merge(pos_cash_balance_data_grouped, how = 'left', left_on = 'SK_ID_PREV', right_on = 'SK_ID_PREV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bureau Data\n",
    "app_train_merged = train_clean.merge(bureau_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')\n",
    "app_test_merged = test_clean.merge(bureau_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prev App Data\n",
    "prev_app_data_grouped = prev_app_data_merged.drop('SK_ID_PREV', axis = 1).groupby('SK_ID_CURR', as_index=False).sum()\n",
    "\n",
    "app_train_merged = app_train_merged.merge(prev_app_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')\n",
    "app_test_merged = app_test_merged.merge(prev_app_data_grouped, how = 'left', left_on = 'SK_ID_CURR', right_on = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 368)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_merged.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 367)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (359, 307511), indices imply (367, 307511)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-0ef1b181f971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_train_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapp_train_merged_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_train_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp_train_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#app_test_merged_imputed = pd.DataFrame(imputer.transform(app_test_merged), columns = app_train_merged.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MikeTodisco/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 361\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MikeTodisco/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MikeTodisco/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4630\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4631\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MikeTodisco/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4606\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4607\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4608\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (359, 307511), indices imply (367, 307511)"
     ]
    }
   ],
   "source": [
    "imputer = Imputer()\n",
    "imputer.fit(app_train_merged.drop('TARGET', axis = 1))\n",
    "app_train_merged_imputed = pd.DataFrame(imputer.transform(app_train_merged.drop('TARGET', axis = 1)), columns = app_train_merged.drop('TARGET', axis = 1).columns)\n",
    "#app_test_merged_imputed = pd.DataFrame(imputer.transform(app_test_merged), columns = app_train_merged.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning final DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_merged = app_train_merged.replace([np.inf, -np.inf], np.nan)\n",
    "app_test_merged = app_test_merged.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filling NA's with 0\n",
    "\n",
    "app_train_merged = app_train_merged.fillna(0).drop(['SK_ID_CURR','TARGET'], axis = 1)\n",
    "app_test_merged = app_test_merged.fillna(0).drop(['SK_ID_CURR'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree = 1)\n",
    "poly_transformer.fit(app_train_merged)\n",
    "train_poly_features = poly_transformer.transform(app_train_merged)\n",
    "\n",
    "train_subset_poly = pd.DataFrame(train_poly_features, columns = poly_transformer.get_feature_names(input_features = app_train_merged.columns.tolist()))\n",
    "\n",
    "test_poly_features = poly_transformer.transform(app_test_merged)\n",
    "test_subset_poly = pd.DataFrame(test_poly_features, columns = poly_transformer.get_feature_names(input_features = app_test_merged.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_poly_features)\n",
    "app_train_scaled = scaler.transform(train_poly_features)\n",
    "app_test_scaled = scaler.transform(test_poly_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(iterations = 10000, random_state = 42, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_model.fit(app_train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(cat_model.predict_proba(app_test_scaled))\n",
    "submission_cat = pd.concat([test.SK_ID_CURR, preds], axis=1).drop(0, axis = 1)\n",
    "submission_cat.columns = ['SK_ID_CURR', 'Target']\n",
    "submission_cat.to_csv('cat_sub3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_subset = train_merged.dropna(thresh=len(train_merged) - 200000000, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_subset.info(verbose=True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_corr_subset = train_merged_subset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column_corr_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_corr = train_merged_subset.corr()['TARGET'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_corr_subset = col_corr[(col_corr >= 0.03) | (col_corr < -0.035)].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_corr_subset.remove('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_corr_subset.remove('CODE_GENDER_XNA')\n",
    "#column_corr_subset.remove('NAME_FAMILY_STATUS_Unknown')\n",
    "#column_corr_subset.remove('NAME_INCOME_TYPE_Maternity leave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_merged_subset[column_corr_subset]\n",
    "\n",
    "test_subset = test_merged[column_corr_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "imputer.fit(train_subset)\n",
    "train_merged_imputed = pd.DataFrame(imputer.transform(train_subset), columns = train_subset.columns)\n",
    "test_merged_imputed = pd.DataFrame(imputer.transform(test_subset), columns = train_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_imputed['DAYS_EMPLOYED_^2'] = train_merged_imputed['DAYS_EMPLOYED'] ** 2\n",
    "#train_merged_imputed['AMT_GOODS_PRICE_^2'] = train_merged_imputed['AMT_GOODS_PRICE'] ** 2\n",
    "train_merged_imputed['DAYS_CREDIT^2'] = train_merged_imputed['DAYS_CREDIT'] ** 2\n",
    "#train_merged_imputed['DAYS_CREDIT_median^2'] = train_merged_imputed['DAYS_CREDIT_median'] ** 2\n",
    "train_merged_imputed['DAYS_BIRTH_^2'] = train_merged_imputed['DAYS_BIRTH'] ** 2\n",
    "train_merged_imputed['REGION_RATING_CLIENT_W_CITY_^2'] = train_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 2\n",
    "train_merged_imputed['REGION_RATING_CLIENT_^2'] = train_merged_imputed['REGION_RATING_CLIENT'] ** 2\n",
    "train_merged_imputed['NAME_INCOME_TYPE_Working_^2'] = train_merged_imputed['NAME_INCOME_TYPE_Working'] ** 2\n",
    "train_merged_imputed['DAYS_LAST_PHONE_CHANGE_^2'] = train_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_1_^2'] = train_merged_imputed['EXT_SOURCE_1'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_2_^2'] = train_merged_imputed['EXT_SOURCE_2'] ** 2\n",
    "train_merged_imputed['EXT_SOURCE_3_^2'] = train_merged_imputed['EXT_SOURCE_3'] ** 2\n",
    "train_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^2'] = train_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 2\n",
    "train_merged_imputed['CODE_GENDER_F_^2']= train_merged_imputed['CODE_GENDER_F'] ** 2\n",
    "\n",
    "train_merged_imputed['DAYS_EMPLOYED_^3'] = train_merged_imputed['DAYS_EMPLOYED'] ** 3\n",
    "#train_merged_imputed['AMT_GOODS_PRICE_^3'] = train_merged_imputed['AMT_GOODS_PRICE'] ** 3\n",
    "train_merged_imputed['DAYS_CREDIT^3'] = train_merged_imputed['DAYS_CREDIT'] ** 3\n",
    "#train_merged_imputed['DAYS_CREDIT_median^3'] = train_merged_imputed['DAYS_CREDIT_median'] ** 3\n",
    "train_merged_imputed['DAYS_BIRTH_^3'] = train_merged_imputed['DAYS_BIRTH'] ** 3\n",
    "train_merged_imputed['REGION_RATING_CLIENT_W_CITY_^3'] = train_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 3\n",
    "train_merged_imputed['REGION_RATING_CLIENT_^3'] = train_merged_imputed['REGION_RATING_CLIENT'] ** 3\n",
    "train_merged_imputed['NAME_INCOME_TYPE_Working_^3'] = train_merged_imputed['NAME_INCOME_TYPE_Working'] ** 3\n",
    "train_merged_imputed['DAYS_LAST_PHONE_CHANGE_^3'] = train_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_1_^3'] = train_merged_imputed['EXT_SOURCE_1'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_2_^3'] = train_merged_imputed['EXT_SOURCE_2'] ** 3\n",
    "train_merged_imputed['EXT_SOURCE_3_^3'] = train_merged_imputed['EXT_SOURCE_3'] ** 3\n",
    "train_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^3'] = train_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 3\n",
    "train_merged_imputed['CODE_GENDER_F_^3']= train_merged_imputed['CODE_GENDER_F'] ** 3\n",
    "\n",
    "test_merged_imputed['DAYS_EMPLOYED_^2'] = test_merged_imputed['DAYS_EMPLOYED'] ** 2\n",
    "#test_merged_imputed['AMT_GOODS_PRICE_^2'] = test_merged_imputed['AMT_GOODS_PRICE'] ** 2\n",
    "test_merged_imputed['DAYS_CREDIT_^2'] = test_merged_imputed['DAYS_CREDIT'] ** 2\n",
    "#test_merged_imputed['DAYS_CREDIT_median^2'] = test_merged_imputed['DAYS_CREDIT_median'] ** 2\n",
    "test_merged_imputed['DAYS_BIRTH_^2'] = test_merged_imputed['DAYS_BIRTH'] ** 2\n",
    "test_merged_imputed['REGION_RATING_CLIENT_W_CITY_^2'] = test_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 2\n",
    "test_merged_imputed['REGION_RATING_CLIENT_^2'] = test_merged_imputed['REGION_RATING_CLIENT'] ** 2\n",
    "test_merged_imputed['NAME_INCOME_TYPE_Working_^2'] = test_merged_imputed['NAME_INCOME_TYPE_Working'] ** 2\n",
    "test_merged_imputed['DAYS_LAST_PHONE_CHANGE_^2'] = test_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_1_^2'] = test_merged_imputed['EXT_SOURCE_1'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_2_^2'] = test_merged_imputed['EXT_SOURCE_2'] ** 2\n",
    "test_merged_imputed['EXT_SOURCE_3_^2'] = test_merged_imputed['EXT_SOURCE_3'] ** 2\n",
    "test_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^2'] = test_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 2\n",
    "test_merged_imputed['CODE_GENDER_F_^2']= test_merged_imputed['CODE_GENDER_F'] ** 2\n",
    "\n",
    "test_merged_imputed['DAYS_EMPLOYED_^3'] = test_merged_imputed['DAYS_EMPLOYED'] ** 3\n",
    "#test_merged_imputed['AMT_GOODS_PRICE_^3'] = test_merged_imputed['AMT_GOODS_PRICE'] ** 3\n",
    "test_merged_imputed['DAYS_CREDIT^3'] = test_merged_imputed['DAYS_CREDIT'] ** 3\n",
    "#test_merged_imputed['DAYS_CREDIT_median^3'] = test_merged_imputed['DAYS_CREDIT_median'] ** 3\n",
    "test_merged_imputed['DAYS_BIRTH_^3'] = test_merged_imputed['DAYS_BIRTH'] ** 3\n",
    "test_merged_imputed['REGION_RATING_CLIENT_W_CITY_^3'] = test_merged_imputed['REGION_RATING_CLIENT_W_CITY'] ** 3\n",
    "test_merged_imputed['REGION_RATING_CLIENT_^3'] = test_merged_imputed['REGION_RATING_CLIENT'] ** 3\n",
    "test_merged_imputed['NAME_INCOME_TYPE_Working_^3'] = test_merged_imputed['NAME_INCOME_TYPE_Working'] ** 3\n",
    "test_merged_imputed['DAYS_LAST_PHONE_CHANGE_^3'] = test_merged_imputed['DAYS_LAST_PHONE_CHANGE'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_1_^3'] = test_merged_imputed['EXT_SOURCE_1'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_2_^3'] = test_merged_imputed['EXT_SOURCE_2'] ** 3\n",
    "test_merged_imputed['EXT_SOURCE_3_^3'] = test_merged_imputed['EXT_SOURCE_3'] ** 3\n",
    "test_merged_imputed['NAME_EDUCATION_TYPE_Higher education_^3'] = test_merged_imputed['NAME_EDUCATION_TYPE_Higher education'] ** 3\n",
    "test_merged_imputed['CODE_GENDER_F_^3']= test_merged_imputed['CODE_GENDER_F'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_transformer = PolynomialFeatures(degree = 1)\n",
    "poly_transformer.fit(train_merged_imputed)\n",
    "train_poly_features = poly_transformer.transform(train_merged_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_poly = pd.DataFrame(train_poly_features, columns = poly_transformer.get_feature_names(\n",
    "    input_features = train_merged_imputed.columns.tolist()\n",
    "))\n",
    "\n",
    "test_poly_features = poly_transformer.transform(test_merged_imputed)\n",
    "test_subset_poly = pd.DataFrame(test_poly_features, columns = poly_transformer.get_feature_names(input_features = test_merged_imputed.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_subset_poly)\n",
    "train_scaled = scaler.transform(train_subset_poly)\n",
    "test_scaled = scaler.transform(test_subset_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(iterations = 1000, random_state = 42, learning_rate = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_model.fit(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#xgb_model = XGBClassifier(n_estimators = 500, silent=True, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model.fit(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_cat = pd.DataFrame(xgb_model.predict_proba(test_scaled))\n",
    "submission_cat = pd.concat([test.SK_ID_CURR, test_y_cat], axis=1).drop(0, axis = 1)\n",
    "submission_cat.columns = ['SK_ID_CURR', 'Target']\n",
    "submission_cat.to_csv('xgb_model10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_cat['Target'] = submission_cat['Target'].apply(lambda x: (x - min_sub_cat) / (max_sub_cat - min_sub_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_submission = pd.read_csv('cat_lr75.csv')\n",
    "xgb_submission = pd.read_csv('xgb1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat['Target'] = (submission_cat['Target']+ xgb_submission['Target']) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.to_csv('combined11.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lightgbm.Dataset(train_scaled, train.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "#                       valid_sets=test_data,\n",
    "                       num_boost_round=10000,\n",
    "#                       early_stopping_rounds=100\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(model.predict(test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbm = pd.concat([test.SK_ID_CURR, preds], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lgbm.columns = ['SK_ID_CURR', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.to_csv('lgbm2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10 = pd.read_csv('combined10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10['Target'] = (combined_10['Target'] + submission_cat['Target']) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_10.to_csv('new_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
